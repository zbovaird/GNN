{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fb8e9e-b169-4462-bb3a-00a71801e619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading time: 29.90 seconds\n",
      "Unique labels in the dataset: ['BENIGN' 'DDoS' 'PortScan' 'Bot' 'Infiltration'\n",
      " 'Web Attack � Brute Force' 'Web Attack � XSS'\n",
      " 'Web Attack � Sql Injection' 'FTP-Patator' 'SSH-Patator' 'DoS slowloris'\n",
      " 'DoS Slowhttptest' 'DoS Hulk' 'DoS GoldenEye' 'Heartbleed']\n",
      "Label distribution in the dataset:\n",
      "Label\n",
      "BENIGN                        2273097\n",
      "DoS Hulk                       231073\n",
      "PortScan                       158930\n",
      "DDoS                           128027\n",
      "DoS GoldenEye                   10293\n",
      "FTP-Patator                      7938\n",
      "SSH-Patator                      5897\n",
      "DoS slowloris                    5796\n",
      "DoS Slowhttptest                 5499\n",
      "Bot                              1966\n",
      "Web Attack � Brute Force         1507\n",
      "Web Attack � XSS                  652\n",
      "Infiltration                       36\n",
      "Web Attack � Sql Injection         21\n",
      "Heartbleed                         11\n",
      "Name: count, dtype: int64\n",
      "Conversion to numeric time: 0.67 seconds\n",
      "Filling NaNs time: 0.55 seconds\n",
      "Handling infinities time: 0.62 seconds\n",
      "Final NaN check and fill time: 0.16 seconds\n",
      "Unique labels in the sampled dataset: ['BENIGN' 'DoS Hulk' 'DDoS' 'PortScan' 'DoS slowloris' 'DoS GoldenEye'\n",
      " 'FTP-Patator' 'DoS Slowhttptest' 'Bot' 'SSH-Patator'\n",
      " 'Web Attack � Brute Force' 'Web Attack � XSS' 'Heartbleed'\n",
      " 'Web Attack � Sql Injection' 'Infiltration']\n",
      "Label distribution in the sampled dataset:\n",
      "Label\n",
      "BENIGN                        454434\n",
      "DoS Hulk                       46486\n",
      "PortScan                       31711\n",
      "DDoS                           25583\n",
      "DoS GoldenEye                   2074\n",
      "FTP-Patator                     1612\n",
      "DoS slowloris                   1173\n",
      "SSH-Patator                     1173\n",
      "DoS Slowhttptest                1072\n",
      "Bot                              390\n",
      "Web Attack � Brute Force         308\n",
      "Web Attack � XSS                 119\n",
      "Web Attack � Sql Injection         7\n",
      "Infiltration                       5\n",
      "Heartbleed                         2\n",
      "Name: count, dtype: int64\n",
      "Normalization time: 0.76 seconds\n",
      "Conversion to PyTorch tensors completed.\n",
      "Constructing k-nearest neighbors graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/566149 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from scipy.sparse import coo_matrix\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'CIC_data.csv'  # Update this with the actual path to your CSV file\n",
    "start_time = time.time()\n",
    "data = pd.read_csv(file_path, low_memory=False)\n",
    "print(f\"Data loading time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Strip any leading or trailing whitespace from the column names\n",
    "data.columns = data.columns.str.strip()\n",
    "\n",
    "# Strip whitespace from label values\n",
    "data['Label'] = data['Label'].str.strip()\n",
    "\n",
    "# Verify unique labels and their distribution\n",
    "unique_labels = data['Label'].unique()\n",
    "print(f\"Unique labels in the dataset: {unique_labels}\")\n",
    "\n",
    "label_counts = data['Label'].value_counts()\n",
    "print(\"Label distribution in the dataset:\")\n",
    "print(label_counts)\n",
    "\n",
    "# Sample a fraction of the rows (e.g., 40%)\n",
    "data_sampled = data.sample(frac=0.2, random_state=42)\n",
    "\n",
    "# Convert all columns to numeric, coerce errors to NaN\n",
    "start_time = time.time()\n",
    "data_numeric = data_sampled.apply(pd.to_numeric, errors='coerce')\n",
    "print(f\"Conversion to numeric time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Fill NaN values with the mean of each column\n",
    "start_time = time.time()\n",
    "data_filled = data_numeric.fillna(data_numeric.mean())\n",
    "print(f\"Filling NaNs time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Handle infinite and very large values\n",
    "start_time = time.time()\n",
    "data_filled = data_filled.replace([np.inf, -np.inf], np.nan)\n",
    "data_filled = data_filled.fillna(data_filled.max())\n",
    "print(f\"Handling infinities time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Check again for any remaining NaNs and fill them\n",
    "start_time = time.time()\n",
    "if data_filled.isnull().values.any():\n",
    "    data_filled = data_filled.fillna(0)\n",
    "print(f\"Final NaN check and fill time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Extract labels\n",
    "labels = data_sampled['Label']\n",
    "data_filled = data_filled.drop(columns=['Label'])\n",
    "\n",
    "# Print unique labels and their counts again after sampling\n",
    "unique_labels = labels.unique()\n",
    "print(f\"Unique labels in the sampled dataset: {unique_labels}\")\n",
    "\n",
    "label_counts = labels.value_counts()\n",
    "print(\"Label distribution in the sampled dataset:\")\n",
    "print(label_counts)\n",
    "\n",
    "# Ensure that there are at least two unique labels\n",
    "if len(unique_labels) <= 1:\n",
    "    raise ValueError(\"The dataset contains only one class. Add more data with different classes to train the model.\")\n",
    "\n",
    "# Normalize the data\n",
    "start_time = time.time()\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data_filled)\n",
    "print(f\"Normalization time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "node_features = torch.tensor(data_scaled, dtype=torch.float32)\n",
    "print(\"Conversion to PyTorch tensors completed.\")\n",
    "\n",
    "# Create a k-nearest neighbors graph with tqdm progress bar\n",
    "start_time = time.time()\n",
    "k = 5\n",
    "print(\"Constructing k-nearest neighbors graph...\")\n",
    "with tqdm(total=data_scaled.shape[0]) as pbar:\n",
    "    knn_graph = kneighbors_graph(data_scaled, k, mode='connectivity', include_self=False)\n",
    "    pbar.update(data_scaled.shape[0])\n",
    "print(f\"k-nearest neighbors graph construction time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Convert knn_graph to COO format\n",
    "knn_graph_coo = coo_matrix(knn_graph)\n",
    "\n",
    "# Create edge_index directly from COO format\n",
    "edge_index = torch.tensor([knn_graph_coo.row, knn_graph_coo.col], dtype=torch.long)\n",
    "print(f\"Edge index shape: {edge_index.shape}\")\n",
    "\n",
    "# Create the PyTorch Geometric data object\n",
    "graph_data = Data(x=node_features, edge_index=edge_index)\n",
    "\n",
    "# Convert labels to numeric\n",
    "label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "labels_numeric = labels.map(label_mapping).values\n",
    "labels_tensor = torch.tensor(labels_numeric, dtype=torch.long)\n",
    "\n",
    "# Add labels to the graph data\n",
    "graph_data.y = labels_tensor\n",
    "\n",
    "# Check label distribution\n",
    "print(\"Label distribution in the dataset:\")\n",
    "print(pd.Series(labels_numeric).value_counts())\n",
    "\n",
    "# Create a train/test split\n",
    "train_mask = torch.rand(node_features.size(0)) < 0.8\n",
    "test_mask = ~train_mask\n",
    "graph_data.train_mask = train_mask\n",
    "graph_data.test_mask = test_mask\n",
    "\n",
    "print(f\"Train size: {graph_data.train_mask.sum()}, Test size: {graph_data.test_mask.sum()}\")\n",
    "\n",
    "# Define the GraphSAGE model\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Initialize the model\n",
    "model = GraphSAGE(in_channels=node_features.size(1), hidden_channels=128, out_channels=len(label_mapping))\n",
    "print(model)\n",
    "\n",
    "# Define the loss and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Timing the training process\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(graph_data)\n",
    "    loss = criterion(out[graph_data.train_mask], graph_data.y[graph_data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Test function\n",
    "def test():\n",
    "    model.eval()\n",
    "    out = model(graph_data)\n",
    "    pred = out.argmax(dim=1)\n",
    "    correct = (pred[graph_data.test_mask] == graph_data.y[graph_data.test_mask]).sum()\n",
    "    acc = int(correct) / int(graph_data.test_mask.sum())\n",
    "    return acc\n",
    "\n",
    "# Training for a number of epochs\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    if epoch % 10 == 0:  # Every 10 epochs, evaluate the model on the test set\n",
    "        acc = test()\n",
    "        print(f'Epoch: {epoch}, Loss: {loss:.4f}, Test Accuracy: {acc:.4f}')\n",
    "\n",
    "# Save the model\n",
    "#torch.save(model.state_dict(), 'graphsage_model.pth')\n",
    "#print(\"Model saved to graphsage_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a886ee49-6ed0-4da5-8826-25805be7cfb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
